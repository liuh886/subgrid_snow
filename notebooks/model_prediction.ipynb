{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction and evaluation\n",
    "\n",
    "- Predict snow depth by pre-trained regressor.\n",
    "- Validate/compared with other data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) data preparation\n",
    "\n",
    "The data used for snow depth prediction/validation contains the following columns:\n",
    "- (1) ERA5 Monthly to be downscaled: snow depth, wind_aspect_factor.\n",
    "- (2) raw data calculated from DTM10, namely raw_pts: terrain parameters.\n",
    "- (3) Senorge monthly snow depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) ERA5 monthly data\n",
    "import pandas as pd\n",
    "from xsnow.goregression import ERA5\n",
    "\n",
    "# load monthly era5\n",
    "era_monthly = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\EAR5_land\\monthly_data_08_22.nc'\n",
    "era = ERA5(era_monthly)\n",
    "# Calculate wind_aspect_factor_index\n",
    "era.cal_wind_aspect_factor_yearly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) raw data\n",
    "def get_df(df_list,crop=[6649790,6700092,49816,123488],x='E',y='N'):\n",
    "\n",
    "    min_y,max_y,min_x,max_x = crop[0],crop[1],crop[2],crop[3]\n",
    "    df_ = []\n",
    "\n",
    "    for dem in df_list:\n",
    "        df = pd.read_csv(dem)\n",
    "        df = df[(df[y] >= min_y) & (df[y] <= max_y) & (df[x] >= min_x) & (df[x] <= max_x)].copy()\n",
    "        df = df.dropna()\n",
    "        df_.append(df)\n",
    "    return pd.concat(df_, axis=0)\n",
    "\n",
    "# combine multiple csv. These csv are generated from the 10m DEMs.\n",
    "df_list = ['6601_4_10m_z33.csv','6600_1_10m_z33.csv','6700_2_10m_z33.csv','6700_3_10m_z33.csv','6701_3_10m_z33.csv','6600_4_10m_z33.csv']\n",
    "# crop to the area of interest\n",
    "new_df = get_df(df_list,crop=[6650000,6706000,41000,125000]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# (3) senorge data\n",
    "# List of file paths\n",
    "file_paths = [r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\senorge\\sd_2008.nc', \n",
    "              r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\senorge\\sd_2009.nc',\n",
    "              r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\senorge\\sd_2020.nc']\n",
    "\n",
    "# Load datasets\n",
    "se = [xr.open_dataset(fp) for fp in file_paths]\n",
    "# Concatenate along the time dimension\n",
    "se_all = xr.concat(se, dim='time')\n",
    "se_all = ERA5(se_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) downscaling model prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE6\n",
    "\n",
    "from importlib import reload\n",
    "import xsnow.goregression\n",
    "import xsnow.read_frost\n",
    "reload(xsnow.goregression)\n",
    "reload(xsnow.read_frost)\n",
    "from xsnow.goregression import Snow_Distributor\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', 'SettingWithCopyWarning')\n",
    "\n",
    "case_6 = Snow_Distributor(new_df,\n",
    "                          era,\n",
    "                          senorge=se_all,\n",
    "                          features=['E','N','h_te_best_fit','slope', 'aspect', \n",
    "                                  'planc','profc','tpi','tpi_9','tpi_27',\n",
    "                                  'curvature','sde_era','wf_positive', \n",
    "                                  'wf_negative','month'],\n",
    "                          regressor_list = ['sd_dtm1_abserror_250_10_qc_nve_geosegment_abs_feb.json',\n",
    "                                             'sd_dtm10_abserror_250_10_qc_nve_geosegment_abs_feb.json',\n",
    "                                             'sd_cop30_abserror_250_10_qc_nve_geosegment_abs_feb.json',\n",
    "                                             'sd_fab_abserror_250_10_qc_nve_geosegment_abs_feb.json'],\n",
    "                          )\n",
    "\n",
    "case_6.product_time_series(dates=['20080401','20200601'],correction='qm',regression='MAD');\n",
    "case_6.timeseries = case_6.mask_out_lake()\n",
    "\n",
    "# the results is saved in the case_6.timeseries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Evaluation of the model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 mesoscale - Plot in chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import rasterio\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "dem_path = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\nve_08_merge_m.tif' # Path to the DEM TIFF file\n",
    "als = rioxarray.open_rasterio(dem_path)\n",
    "test = case_6.timeseries.sel(time='2008-04-01').rename({'E':'x','N':'y'})\n",
    "test.rio.set_crs(als.rio.crs)\n",
    "als = als.where(als != als.rio.nodata).squeeze()\n",
    "als_m = als.rio.reproject_match(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import map_coordinates\n",
    "import matplotlib.pyplot as plt\n",
    "from xsnow.misc import evaluate_difference\n",
    "reload(xsnow.misc)\n",
    "\n",
    "def extract_along_line(resampled_da1, resampled_da2, resampled_dah, resampled_das,resampled_dae):\n",
    "\n",
    "    # Define the coordinates of the start and end points for each of the five lines\n",
    "    lines = [\n",
    "        ((41000, 6704000), (122100, 6705505)),\n",
    "        ((41000, 6694000), (122100, 6696000)),\n",
    "        ((41000, 6683500), (122100, 6685371)),\n",
    "        ((41000, 6673620), (122100, 6675092)),\n",
    "        ((41000, 6663300), (122100, 6664674)),\n",
    "        ((41000, 6653100), (122100, 6654503)),\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Initialize lists to store the extracted values for each line\n",
    "    values_da1_list = []\n",
    "    values_da2_list = []\n",
    "    values_dah_list = []\n",
    "    values_das_list = []\n",
    "    values_dae_list = []\n",
    "\n",
    "    x_list = []\n",
    "\n",
    "    for line in lines:\n",
    "        (x_start, y_start), (x_end, y_end) = line\n",
    "\n",
    "        # Create a grid of coordinates along the line\n",
    "        x_line = np.arange(x_start, x_end + 10, 10)\n",
    "        y_line = np.arange(y_start, y_end + 10, 10)\n",
    "\n",
    "        # Interpolate values from the resampled DataArrays using map_coordinates\n",
    "        # return values from a grid in resolution 10m\n",
    "        values_da1 = resampled_da1.interp(x=x_line, y=y_line, method='linear')\n",
    "        values_da2 = resampled_da2.interp(x=x_line, y=y_line, method='linear')\n",
    "        values_dah = resampled_dah.interp(x=x_line, y=y_line, method='nearest')\n",
    "        values_das = resampled_das.interp(x=x_line, y=y_line, method='linear')\n",
    "        values_dae = resampled_dae.interp(x=x_line, y=y_line, method='linear')\n",
    "\n",
    "        # mask by da2\n",
    "        values_da2 = values_da2.where(values_da1.notnull())\n",
    "        values_da1 = values_da1.where(values_da2 > 0)\n",
    "        #values_dah = values_dah.where(values_da2.notnull())\n",
    "        values_das = values_das.where(values_da1.notnull())\n",
    "\n",
    "        # Append the extracted values for each line to the respective lists\n",
    "        values_da1_list.append(values_da1)\n",
    "        values_da2_list.append(values_da2)\n",
    "        values_dah_list.append(values_dah)\n",
    "        values_das_list.append(values_das)\n",
    "        values_dae_list.append(values_dae)\n",
    "\n",
    "        x_list.append(x_line)\n",
    "\n",
    "    # Now, values_da1_list and values_da2_list contain the extracted values for each of the five lines using map_coordinates interpolation.\n",
    "    return values_da1_list, values_da2_list, values_dah_list, values_das_list, values_dae_list, x_list\n",
    "\n",
    "def plot_scatter_snow_depth(values_da1_list, \n",
    "                            values_da2_list, \n",
    "                            values_dah_list, \n",
    "                            values_das_list,\n",
    "                            values_dae_list,\n",
    "                            x_list,\n",
    "                            window_size_x=10,\n",
    "                            title='snowdepth_senoreg_scatter.jpg'):\n",
    "    \n",
    "    # Create a figure with five subplots\n",
    "    fig, axs = plt.subplots(6, 1, figsize=(15, 9))  # Adjust figsize as needed\n",
    "    \n",
    "    color = [\"#1f77b4\", # Strong blue for R1\n",
    "            \"#ff7f0e\", # Strong orange for A1\n",
    "            \"#e7ba52\", # Lighter orange for A2\n",
    "            \"#2ca02c\", # Strong green for B1\n",
    "            \"#17becf\", # Lighter blue for B2\n",
    "            \"#b3b3b3\"  # Muted grey for R2\n",
    "            ]\n",
    "\n",
    "    data_m = {}\n",
    "    data_s = {}\n",
    "\n",
    "    # Loop through each line's extracted values and plot them as scatter plots\n",
    "    for i, (values_da1, values_da2, values_dah,values_das,values_dae, x) in enumerate(zip(values_da1_list, values_da2_list, values_dah_list,values_das_list,values_dae_list,x_list)):\n",
    "        ax = axs[i]\n",
    "        ax.set_title('')\n",
    "        ax.label_outer()\n",
    "        ax.set_ylim((0,8))\n",
    "        ax.set_xlim(min(x), max(x))\n",
    "\n",
    "        da1 = values_da1.mean(dim='y',skipna=True).rolling(x=window_size_x, center=True).mean()\n",
    "        da2 = values_da2.mean(dim='y',skipna=True).rolling(x=window_size_x, center=True).mean()\n",
    "        das = values_das.mean(dim='y',skipna=True).rolling(x=window_size_x, center=True).mean()\n",
    "        dae = values_dae.mean(dim='y',skipna=True).rolling(x=window_size_x, center=True).mean()\n",
    "\n",
    "        # Scatter plot for DataArray z\n",
    "        # add the secondaries y-axis\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.grid(False)\n",
    "        ax2.plot(x, \n",
    "                    values_dah.mean(dim='y'), \n",
    "                    c='#1c222b',linestyle = \"--\", linewidth=1, alpha=0.5, \n",
    "                    label='Elevation')\n",
    "        #ax2.set_ylim((500, 2000))\n",
    "\n",
    "        ax.plot(x, \n",
    "                   dae, \n",
    "                   c='red',linestyle = \":\", linewidth=2, alpha=0.5, \n",
    "                   label='ERA5 Land')\n",
    "\n",
    "        # Scatter plot for DataArray 3\n",
    "        ax.scatter(x, \n",
    "                   das, \n",
    "                   c=color[5],marker = \".\", s=3, alpha=1, \n",
    "                   label='seNorge')\n",
    "        ax.grid(False)\n",
    "\n",
    "        # Scatter plot for DataArray 2\n",
    "        ax.scatter(x, \n",
    "                   da2, \n",
    "                   c=color[0],marker = \".\", s=2, alpha=0.7, \n",
    "                   label='ALS Snow survey')\n",
    "\n",
    "        # Scatter plot for DataArray 1\n",
    "        ax.scatter(x, \n",
    "                   da1, \n",
    "                   c=color[1],marker = \".\", s=2, alpha=0.5, \n",
    "                   label='Downscaled output')\n",
    "        \n",
    "\n",
    "        # statistics\n",
    "        spearman_m, ksd_m, r2_m, rmse_m = evaluate_difference(da1, da2)\n",
    "        spearman_s, ksd_s, r2_s, rmse_s = evaluate_difference(das, da2)\n",
    "        data_m[str(i+2)+' fl.'] = [r2_m,ksd_m,spearman_m, rmse_m]\n",
    "        data_s[str(i+2)+' fl.'] = [r2_s,ksd_s,spearman_s, rmse_s]\n",
    "\n",
    "        text_str = f\"Downscaled output ($R^2$, RMSE):{r2_m:.2f}, {rmse_m:.2f} m\\nSeNorge ($R^2$, RMSE): {r2_s:.2f}, {rmse_s:.2f} m\"\n",
    "        ax.text(0.65, 0.90, text_str, transform=ax.transAxes, ha='left', va='top')\n",
    "\n",
    "\n",
    "        # set legend\n",
    "        if i == 0:\n",
    "            # Combine legends for both y-axes\n",
    "            lines1, labels1 = ax.get_legend_handles_labels()\n",
    "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "            order = [2,3,1,0]\n",
    "            # Create a legend at the top of the figure\n",
    "            fig.legend([lines1[idx] for idx in order] + lines2, [labels1[idx] for idx in order]  + labels2, markerscale=10, loc='upper center', bbox_to_anchor=(0.4, 1.06), ncol=2)\n",
    "\n",
    "\n",
    "        # set the y-axis label\n",
    "        if i == 2:\n",
    "            ax2.set_ylabel('Elevation [m]')\n",
    "            ax.set_ylabel('Snow depth [m ]')\n",
    "\n",
    "            #ax.scatter(w_x,w_y,c='r',s=10,marker='x',label='MOGEN')\n",
    "\n",
    "    # Set x labels \n",
    "    ax.set_xlabel('East [m]')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.1) \n",
    "    plt.savefig(title, dpi=600)\n",
    "    # Show the figure\n",
    "    plt.show()\n",
    "\n",
    "    return data_m,data_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN SNOW DEPTH DATA from senorge\n",
    "import xarray as xr\n",
    "reload(xsnow.goregression)\n",
    "from xsnow.goplot import plot_metrics\n",
    "from xsnow.goregression import quantile_mapping_xr\n",
    "import pickle\n",
    "\n",
    "#senorge = xr.open_dataset(r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\senorge\\sd_2008.nc')\n",
    "#senorge_april = senorge.sel(time=slice('2008-04-01', '2008-04-30'),x=slice(41000, 130243), y=slice(6600000, 7200000))\n",
    "#\n",
    "test = case_6.timeseries.sel(time='2008-04-01').rename({'E':'x','N':'y'})\n",
    "\n",
    "with open('adjust_factor_08_case6.pkl', 'rb') as f:  # adjust_factor_08_test is from \n",
    "    adjust_factor = pickle.load(f )\n",
    "    for als, factor in adjust_factor.items():\n",
    "        quantile_mapping_xr(test,factor['delta_lt1'],factor['delta_gt1'],dem=als, split=0,offset=0)\n",
    "        #quantile_mapping_original_xr(test,factor['delta_gt1'],factor['dem_q_gt1'],dem=als, split=0,offset=0)\n",
    "\n",
    "# Usage\n",
    "v1_list, v2_list, vh_list, vs_list,ve_list, x_list = extract_along_line(test.sd_predict_dtm1_.where(test.lake_mask == 0),\n",
    "                                                               als_m.where(als_m > 0),\n",
    "                                                               test.z,\n",
    "                                                               senorge_april.snow_depth.mean(dim='time') / 100,\n",
    "                                                               test.sde_era)\n",
    "\n",
    "data_m,data_s = plot_scatter_snow_depth(v1_list, v2_list, vh_list, vs_list,ve_list,x_list,\n",
    "                                        window_size_x=10,\n",
    "                                        title='snowdepth_senoreg_scatter_08_case6_10_corr.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_list, v2_list, vh_list, vs_list,ve_list, x_list = extract_along_line(test.sd_predict_dtm1.where(test.lake_mask == 0) -1,\n",
    "                                                               als_m.where(als_m > 0),\n",
    "                                                               test.z,\n",
    "                                                               senorge_april.snow_depth.mean(dim='time') / 100,\n",
    "                                                               test.sde_era)\n",
    "\n",
    "data_m,data_s = plot_scatter_snow_depth(v1_list, v2_list, vh_list, vs_list,ve_list,x_list,\n",
    "                                        window_size_x=10,\n",
    "                                        title='snowdepth_senoreg_scatter_08_case6_10_original.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import rasterio\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pickle\n",
    "reload(xsnow.goregression)\n",
    "from xsnow.goregression import quantile_mapping_original_xr, quantile_mapping_xr\n",
    "import xarray as xr\n",
    "\n",
    "# Senorge 08\n",
    "senorge = xr.open_dataset(r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\senorge\\sd_2008.nc')\n",
    "senorge_april = senorge.sel(time=slice('2008-04-01', '2008-04-30'),x=slice(41000, 130243), y=slice(6600000, 7200000))\n",
    "\n",
    "# ALS 08\n",
    "dem_path = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\nve_08_merge_m.tif' # Path to the DEM TIFF file\n",
    "als = rioxarray.open_rasterio(dem_path)\n",
    "test = case_6.timeseries.sel(time='2008-04-01').rename({'E':'x','N':'y'})\n",
    "test.rio.set_crs(als.rio.crs)\n",
    "als = als.where(als != als.rio.nodata).squeeze()\n",
    "als_m = als.rio.reproject_match(test)\n",
    "\n",
    "# add correct value for test (xr.Dataset)\n",
    "with open('adjust_factor_08_case6.pkl', 'rb') as f:\n",
    "    adjust_factor = pickle.load(f)\n",
    "    for als, factor in adjust_factor.items():\n",
    "        #quantile_mapping_original_xr(test,factor['delta_gt1'],factor['dem_q_gt1'],dem=dem, split=0)\n",
    "        quantile_mapping_xr(test,factor['delta_lt1'],factor['delta_gt1'],dem=als, split=0)\n",
    "\n",
    "def plot_all_meso(test,dem_m,senorge_april,mask,\n",
    "                  year='08_',case='_case7',\n",
    "                  cols=['sd_predict_dtm1','sd_predict_dtm10','sd_predict_cop30','sd_predict_fab']):\n",
    "\n",
    "    table_08 = []\n",
    "\n",
    "    for dem in cols:\n",
    "        v1_list, v2_list, vh_list, vs_list,ve_list,x_list = extract_along_line(test[dem].where(mask.rename({'E':'x','N':'y'}) == 0) -1,\n",
    "                                                                       dem_m.where(dem_m > 0),\n",
    "                                                                       test.z,\n",
    "                                                                       senorge_april.snow_depth.mean(dim='time') / 100,\n",
    "                                                                       test.sde_era)\n",
    "        data_m,data_s = plot_scatter_snow_depth(v1_list, \n",
    "                                                v2_list, \n",
    "                                                vh_list, \n",
    "                                                vs_list,\n",
    "                                                ve_list,\n",
    "                                                x_list,\n",
    "                                                window_size_x=10,\n",
    "                                                title='snowdepth_senoreg_scatter_' + year + dem.split('_')[2] + case + '.jpg');\n",
    "        \n",
    "        table = pd.DataFrame(data_m, index=['r_2', 'ksd', 'spearman', 'rmse']).transpose().melt(var_name='metrics', value_name='value')\n",
    "        table['dataset'] = dem.split('_')[2].upper()\n",
    "        table_08.append(table)\n",
    "    table_s = pd.DataFrame(data_s, index=['r_2', 'ksd', 'spearman', 'rmse']).transpose().melt(var_name='metrics', value_name='value')\n",
    "    table_s['dataset'] = 'seNorge'\n",
    "    table_08.append(table_s)\n",
    "    \n",
    "    return table_08\n",
    "\n",
    "table_08 = plot_all_meso(test,\n",
    "                         als_m, \n",
    "                         senorge_april, \n",
    "                         mask,\n",
    "                         year='08_',\n",
    "                         case='_case6_10_original');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS 09\n",
    "dem_path = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\nve_09_merge_m.tif' # Path to the DEM TIFF file\n",
    "als = rioxarray.open_rasterio(dem_path)\n",
    "test_09 = case_6.timeseries.sel(time='2009-04-01').rename({'E':'x','N':'y'})\n",
    "test_09.rio.set_crs(als.rio.crs)\n",
    "als = als.where(als != als.rio.nodata).squeeze()\n",
    "dem_sd_09 = als.rio.reproject_match(test_09)\n",
    "\n",
    "# add correct value for test (xr.Dataset)\n",
    "with open('adjust_factor_09_case6.pkl', 'rb') as f:\n",
    "    adjust_factor = pickle.load(f)\n",
    "    for als, factor in adjust_factor.items():\n",
    "        #quantile_mapping_original_xr(test_09,factor['delta_gt1'],factor['dem_q_gt1'],dem=dem, split=0)\n",
    "         quantile_mapping_xr(test_09,factor['delta_lt1'],factor['delta_gt1'],dem=als, split=0)\n",
    "         \n",
    "# Senorge 09\n",
    "senorge_09 = xr.open_dataset(r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\senorge\\sd_2009.nc')\n",
    "senorge_april_09 = senorge_09.sel(time=slice('2009-04-01', '2009-04-30'),x=slice(41000, 130243), y=slice(6600000, 7200000))\n",
    "\n",
    "table_09 = plot_all_meso(test_09,\n",
    "                         dem_sd_09, \n",
    "                         senorge_april_09,\n",
    "                         mask,\n",
    "                         year='09_',\n",
    "                         case='_case6_10_corr');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_08 = pd.concat(table_08)\n",
    "df_08['yaer'] = '2008'\n",
    "df_09 = pd.concat(table_09)\n",
    "df_09['yaer'] = '2009'\n",
    "df_08.reset_index(drop=True, inplace=True)\n",
    "df_09.reset_index(drop=True, inplace=True)\n",
    "\n",
    "metric_name_mapping = {\n",
    "    \"r_2\": \"$R^2$\",\n",
    "    \"spearman\": \"$\\\\rho$\",\n",
    "    \"rmse\": \"RMSE [m]\",\n",
    "    \"ksd\": \"KSD\",\n",
    "}\n",
    "\n",
    "df_08['metrics'] = df_08['metrics'].map(metric_name_mapping)\n",
    "df_09['metrics'] = df_09['metrics'].map(metric_name_mapping)\n",
    "\n",
    "def plot_metrics_chart(df_08, label='2008', title='snowdepth_senoreg_scatter_08_case6_metrics.jpg'):\n",
    "    # Define the desired order of the metrics\n",
    "    desired_order = [\"$R^2$\", \"$\\\\rho$\", \"RMSE [m]\", \"KSD\"]\n",
    "    #custom_palette = [sns.set_hls_values(color, l=0.3,s=0.7) for color in [\"lime\",'olive','crimson','royalblue','orange']]\n",
    "    custom_palette = [#\"#1f77b4\", # Strong blue for R1\n",
    "            \"#ff7f0e\", # Strong orange for A1\n",
    "            \"#e7ba52\", # Lighter orange for A2\n",
    "            \"#2ca02c\", # Strong green for B1\n",
    "            \"#17becf\", # Lighter blue for B2\n",
    "            \"#b3b3b3\"  # Muted grey for R2\n",
    "            ]\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Initialize the figure\n",
    "    f, ax = plt.subplots()\n",
    "    sns.despine(bottom=True, left=True)\n",
    "\n",
    "    # Show each observation with a scatterplot\n",
    "    sns.stripplot(\n",
    "        data=df_08, x=\"value\", y=\"metrics\", hue=\"dataset\",\n",
    "        dodge=True, alpha=.35, zorder=1, legend=False, palette=custom_palette,\n",
    "        order=desired_order  # Specify the desired order here\n",
    "    )\n",
    "\n",
    "    # Show the conditional means, aligning each pointplot in the center of the strips\n",
    "    sns.pointplot(\n",
    "        data=df_08, x=\"value\", y=\"metrics\", hue=\"dataset\",\n",
    "        dodge=.8 - .8 / 3, palette=custom_palette, errorbar=None,\n",
    "        markers=\"d\", linestyles=\"none\",\n",
    "        order=desired_order  # Specify the desired order here\n",
    "    )\n",
    "\n",
    "    ax.set(xlim=(0, 1.15))\n",
    "    # Remove the dataset title from the legend\n",
    "    legend = ax.get_legend()\n",
    "    legend.set_title('')\n",
    "\n",
    "    # Improve the legend\n",
    "    sns.move_legend(\n",
    "        ax, loc=\"lower right\", ncol=2, frameon=False, columnspacing=1, handletextpad=0,\n",
    "    )\n",
    "    # Remove y-axis and x-axis labels\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel(label)\n",
    "    plt.tight_layout()  # Add this line to make the plot tight\n",
    "\n",
    "    plt.savefig(title,dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics_chart(df_08,label='2008',title='2008_metrics_case6_10_corr.jpg')\n",
    "plot_metrics_chart(df_09,label='2009',title='2009_metrics_case6_10_corr.jpg')\n",
    "\n",
    "# Assuming you have two DataFrames: df_08 and df_09\n",
    "combined_df = pd.concat([df_08, df_09])\n",
    "# If you want to reset the index of the combined DataFrame\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "# \n",
    "plot_metrics_chart(combined_df,label='Values',title='2008_2009_metrics_case6_10_corr.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 plot snow depth map vs Senorge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as mcm\n",
    "import datashader.transfer_functions as tf\n",
    "from matplotlib.colors import Normalize\n",
    "from xrspatial import hillshade\n",
    "import colorcet as cc\n",
    "\n",
    "def comparison(comparison_08,senorge_april,dem='sd_predict_dtm1_',title=\"snowdepth_senoreg_map.jpg\",cmap='Spectral',alpha=150,vmax=5):\n",
    "\n",
    "    fig = plt.figure(figsize=(13, 8))\n",
    "    gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 0.05])\n",
    "\n",
    "    # Basemap using Datashader and embed it in one of the subplots\n",
    "    terrain = comparison_08.z\n",
    "    illuminated = hillshade(terrain)\n",
    "\n",
    "    img = tf.stack(\n",
    "        tf.shade(terrain, cmap=[\"black\", \"white\"], how=\"linear\"),\n",
    "        tf.shade(illuminated,  cmap=[\"black\", \"white\"], how=\"linear\", alpha=100)\n",
    "    )\n",
    "\n",
    "    # Set the extent based on your data if needed\n",
    "    extent = [comparison_08.x.min(), comparison_08.x.max(), comparison_08.y.min(), comparison_08.y.max()]\n",
    "\n",
    "    # Hide all titles and display the combined basemap\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax = plt.subplot(gs[i, j])\n",
    "            ax.set_title('')\n",
    "            ax.imshow(img.to_pil(), extent=extent)\n",
    "            ax.label_outer()\n",
    "            \n",
    "    if 'lake_mask' in comparison_08:\n",
    "        mask = comparison_08['lake_mask']\n",
    "    else:\n",
    "        mask = comparison_08[dem].where(comparison_08[dem] > 0)\n",
    "\n",
    "    # (0,0) reference ERA5 in 9 KM resolution\n",
    "    p_00 = tf.shade(comparison_08.sde_era, cmap=mcm.get_cmap(cmap), how=\"linear\", alpha=alpha, span=[0, vmax])\n",
    "    ax = plt.subplot(gs[0, 0])\n",
    "    ax.imshow(p_00.to_pil(), extent=extent)\n",
    "\n",
    "    # (0,1) original\n",
    "    p_01 = tf.shade(comparison_08[dem].where(mask==0), cmap=mcm.get_cmap(cmap), how=\"linear\", alpha=alpha, span=[0, vmax])\n",
    "    ax = plt.subplot(gs[0, 1])\n",
    "    ax.imshow(p_01.to_pil(), extent=extent)\n",
    "\n",
    "    # (1,0) plot senorge\n",
    "    p_10 = tf.shade(senorge_april.snow_depth.mean(dim='time') / 100, cmap=mcm.get_cmap(cmap), how=\"linear\", alpha=alpha, span=[0, vmax])\n",
    "    ax = plt.subplot(gs[1, 0])\n",
    "    ax.imshow(p_10.to_pil(), extent=extent)\n",
    "\n",
    "    # (1,1) Coarsen xarray.Dataset into 1 km resolution\n",
    "    p_11 = tf.shade(comparison_08[dem].where(mask==0).coarsen(x=100, y=100, boundary='pad').mean(), cmap=mcm.get_cmap(cmap), how=\"linear\", alpha=alpha, span=[0, 5])\n",
    "    ax = plt.subplot(gs[1, 1])\n",
    "    ax.imshow(p_11.to_pil(), extent=extent)\n",
    "\n",
    "    # Create a colorbar\n",
    "    norm = Normalize(vmin=0, vmax=vmax)  # Adjust the vmin and vmax as needed\n",
    "    scalar_map = plt.cm.ScalarMappable(cmap=mcm.get_cmap(cmap), norm=norm)\n",
    "    scalar_map.set_array([])  # Provide an empty array\n",
    "    cax = plt.subplot(gs[:, 2])\n",
    "    cbar = plt.colorbar(scalar_map, cax=cax, orientation='vertical',shrink=0.5)\n",
    "    cbar.set_label('Snow depth [m]', fontsize=16)  # Customize the label as needed\n",
    "    cbar.ax.set_aspect(5.7)  # Adjust the aspect ratio to control the length\n",
    "\n",
    "    cbar.ax.tick_params(labelsize=14)  # Set label size\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(title, dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "# OPEN SNOW DEPTH DATA from senorge\n",
    "import xarray as xr\n",
    "senorge_08 = xr.open_dataset(r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\senorge\\sd_2008.nc')\n",
    "\n",
    "# 08 choose the dataset\n",
    "comparison(case_6.timeseries.sel(time='2008-04-01').rename({'E':'x','N':'y'}), \n",
    "           senorge_08.sel(time=slice('2008-04-01', '2008-04-30'),x=slice(41000, 125000), y=slice(6650000,6700092)),\n",
    "           dem='sd_predict_dtm1_',\n",
    "           title=\"snowdepth_senoreg_map_08_case6_corr.jpg\",\n",
    "           cmap='Spectral',#'RdYlBu',\n",
    "           alpha=180)\n",
    "\n",
    "# 09\n",
    "#senorge_09 = xr.open_dataset(r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\senorge\\sd_2009.nc')\n",
    "#comparison(case_60.timeseries.sel(time='2009-04-01').rename({'E':'x','N':'y'}), \n",
    "#           senorge_09.sel(time=slice('2009-04-01', '2009-04-30'),x=slice(41000, 125000), y=slice(6650000,6700092)),\n",
    "#           dem='sd_predict_dtm1_',title=\"snowdepth_senoreg_map_09_case6_corr.jpg\",\n",
    "#           cmap='Spectral')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Model prediction visual inspection by Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as mcm\n",
    "import datashader.transfer_functions as tf\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import Normalize\n",
    "from xrspatial import hillshade\n",
    "from matplotlib.ticker import MaxNLocator \n",
    "import numpy as np\n",
    "\n",
    "def comparison_edge(comparison_08,senorge_april=None,cmap='Spectral',dem='sd_predict_dtm1_',offset=0,title=\"snowdepth_senoreg_map.jpg\"):\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Basemap using Datashader and embed it in one of the subplots\n",
    "    terrain = comparison_08.z\n",
    "    illuminated = hillshade(terrain)\n",
    "\n",
    "    # Set the extent based on your data if needed\n",
    "    extent = [comparison_08.x.min(), comparison_08.x.max(), comparison_08.y.min(), comparison_08.y.max()]\n",
    "\n",
    "    img = tf.stack(\n",
    "        tf.shade(terrain, cmap=[\"black\", \"white\"], how=\"linear\"),\n",
    "        tf.shade(illuminated, cmap=[\"black\", \"white\"], how=\"linear\", alpha=150)\n",
    "    )\n",
    "    \n",
    "    ax.imshow(img.to_pil(), extent=extent)\n",
    "    \n",
    "    # Hide all titles and display the combined basemap\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.yaxis.tick_left()\n",
    "    ax.xaxis.labelpad = 30\n",
    "\n",
    "    # Original shade image\n",
    "    #p_01 = tf.shade(comparison_08[dem], cmap=mcm.get_cmap(cmap), how=\"linear\", alpha=150, span=[0, 2])\n",
    "\n",
    "    # Create a custom colormap by modifying the original colormap\n",
    "    custom_cmap = mcm.get_cmap(cmap, 256)\n",
    "    custom_cmap_colors = custom_cmap(np.linspace(0, 1, 256))\n",
    "    custom_cmap_colors[:int(0.055 * 256)] = [1, 1, 1, 1]  # Set values below 0.1 to white\n",
    "    custom_cmap = ListedColormap(custom_cmap_colors)\n",
    "\n",
    "    # Apply the custom colormap to the image\n",
    "    p_01 = tf.shade(comparison_08[dem].where(comparison_08.lake_mask ==0) - offset, cmap=custom_cmap, how=\"linear\", alpha=150, span=[0, 2])\n",
    "\n",
    "    ax.imshow(p_01.to_pil(), extent=extent)\n",
    "\n",
    "    # Create a colorbar using ScalarMappable\n",
    "    norm = Normalize(vmin=0, vmax=2)\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    # Customize the colorbar\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='vertical', shrink=0.5) \n",
    "\n",
    "    # Customize colorbar ticks and label size\n",
    "    cbar.ax.yaxis.set_major_locator(MaxNLocator(nbins=5)) \n",
    "    cbar.ax.tick_params(labelsize=14) \n",
    "    cbar.set_label('Snow depth [m]', fontsize=16)  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(title, dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "comparison_edge(\n",
    "    case_6.timeseries.sel(time='2020-06-01', N=slice(6649500, 6670000), E=slice(98961, 123200)).rename({'E': 'x', 'N': 'y'}),\n",
    "    cmap='Spectral', dem='sd_predict_dtm1', title=\"m_snowdepth_senoreg_map_2020_case_6_original.jpg\",offset=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Micro scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import rasterio\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xsnow.misc import evaluate_difference\n",
    "from xsnow.goplot import RMSE,plot_over_dem\n",
    "#x is between 54000 to 60000\n",
    "import datashader.transfer_functions as tf\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import Normalize\n",
    "from xrspatial import hillshade\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def load_dem_plot_difference(dem_m,\n",
    "                             model,\n",
    "                             model_z,\n",
    "                             xlim=(54000,56000),ylim=(6663946,6664442),\n",
    "                             title='snowdepth_senoreg_profile.jpg'):\n",
    "    \n",
    "    color = [\"#1f77b4\", # Strong blue for R1\n",
    "                \"#ff7f0e\", # Strong orange for A1\n",
    "                \"#e7ba52\", # Lighter orange for A2\n",
    "                \"#2ca02c\", # Strong green for B1\n",
    "                \"#17becf\", # Lighter blue for B2\n",
    "                \"#b3b3b3\"  # Muted grey for R2\n",
    "                ]\n",
    "    \n",
    "    # (1) generate difference\n",
    "    difference = model - dem_m\n",
    "\n",
    "    # (2) Define your hard threshold and Z-score threshold and mask out\n",
    "    hard_threshold = 20\n",
    "    z_score_threshold = 3  # Adjust the Z-score threshold as needed\n",
    "    hard_outliers = difference > hard_threshold\n",
    "    da = difference.where(~hard_outliers)\n",
    "    # Calculate the Z-scores for each data point\n",
    "    z_scores = (da - da.mean()) / da.std()\n",
    "    z_score_outliers = np.abs(z_scores) > z_score_threshold\n",
    "    # Mask out the outliers\n",
    "    combined_outliers = hard_outliers | z_score_outliers\n",
    "    difference_map = difference.where(~combined_outliers)\n",
    "    difference_map = difference_map.sel(x=slice(xlim[0],xlim[1]),y=slice(ylim[0],ylim[1]))\n",
    "    \n",
    "    # (3) Aggregate the difference map to x axis\n",
    "    squeeze_N = difference_map.mean(dim='y')\n",
    "\n",
    "    # (4) Draw profile\n",
    "    n_profile=(ylim[0]+ylim[1])/2\n",
    "\n",
    "    # A. Plot map of ALS\n",
    "    fig, ax = plt.subplots(4,1,figsize=(14, 9),sharex=True,sharey=False,gridspec_kw={'height_ratios': [1, 1, 2, 2]})\n",
    "\n",
    "    plot0 = dem_m.plot(ax=ax[1],cmap='Spectral', vmin=0, vmax=8,add_colorbar=False)\n",
    "    ax[1].set_xlim(xlim[0],xlim[1])\n",
    "    ax[1].set_ylim(ylim[0],ylim[1])\n",
    "\n",
    "    # B. Plot map of model\n",
    "    plot1 =model.plot(ax=ax[2],cmap='Spectral', vmin=0, vmax=8,add_colorbar=False)\n",
    "    ax[2].set_ylim(ylim[0]-(ylim[1]-ylim[0]),ylim[1])\n",
    "\n",
    "    # Create external color bars\n",
    "    fig.subplots_adjust(right=1.00)\n",
    "    cax0 = fig.add_axes([1, 0.45, 0.015, 0.28])  # Adjust these values as needed\n",
    "    cax2 = fig.add_axes([1, 0.17, 0.015, 0.15])  # Adjust these values as needed\n",
    "    cax0.set_label('Snow depth [m]')\n",
    "    cax0.set_label('Difference [m]')\n",
    "\n",
    "    # Create the shared color bar\n",
    "    plt.colorbar(plot1, cax=cax0)\n",
    "    \n",
    "    # C. plot map of difference\n",
    "\n",
    "    # Basemap using Datashader and embed it in one of the subplots\n",
    "    # Set the extent based on your data if needed\n",
    "    extent = [xlim[0], xlim[1], ylim[0]-(ylim[1]-ylim[0]),ylim[1]]\n",
    "    z = model_z.sel(x=slice(xlim[0],xlim[1]),y=slice(ylim[0]-(ylim[1]-ylim[0]),ylim[1]))\n",
    "    illuminated = hillshade(z)\n",
    "    img = tf.stack(\n",
    "        tf.shade(z, cmap=[\"black\", \"white\"], how=\"linear\"),\n",
    "        tf.shade(illuminated, cmap=[\"black\", \"white\"], how=\"linear\", alpha=100)\n",
    "    )\n",
    "    ax[3].imshow(img.to_pil(), extent=extent, aspect='auto')\n",
    "    \n",
    "    # Plotting for ax[3] with its own color bar\n",
    "    plot2 = difference_map.plot(ax=ax[3], cmap='coolwarm', vmin=-4, vmax=4,add_colorbar=False)\n",
    "    ax[3].set_ylim(ylim[0]-(ylim[1]-ylim[0]),ylim[1])\n",
    "    ax[3].set_xlim(xlim[0],xlim[1])\n",
    "    ax[3].tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)\n",
    "\n",
    "    plt.colorbar(plot2, cax=cax2)\n",
    "\n",
    "    # statistics\n",
    "    rmse_map = RMSE(difference_map)\n",
    "    mean_map = np.nanmean(difference_map)\n",
    "    print('rmse of map', rmse_map)\n",
    "    \n",
    "    # D. Profile plot\n",
    "\n",
    "    # Plot variable on primary y-axis\n",
    "    model.sel(y=n_profile, method='nearest').plot(ax=ax[0], color=color[1], label='Downscaled output')\n",
    "    dem_m.sel(y=n_profile, method='nearest').plot(ax=ax[0], color=color[0], label='ALS snow survey')\n",
    "    \n",
    "    # Plot difference on secondary y-axis\n",
    "    ax2 = ax[0].twinx()\n",
    "    \n",
    "    # Plot elevation on secondary y-axis\n",
    "    z.sel(y=n_profile, method='nearest').plot(ax=ax2, color='#1c222b', label='Elevation', linestyle='--',alpha=0.5)\n",
    "\n",
    "    # plot snow depth difference of transect on secondary y-axis\n",
    "    #difference_map.sel(y=n_profile, method='nearest').plot(ax=ax2,color='green', label='profile')\n",
    "    ax2.set_title('')\n",
    "    ax2.set_xlabel('')\n",
    "    ax[0].set_ylim((0,10))\n",
    "    ax[0].set_xlim(xlim[0],xlim[1])\n",
    "    ax[0].grid(False)\n",
    "    ax2.grid(False)\n",
    "    ax2.set_ylabel('Elevation [m]')\n",
    "    ax[1].axhline(y=n_profile, color='white', linestyle='-', linewidth=1)\n",
    "    ax[2].axhline(y=n_profile, color='white', linestyle='-', linewidth=1)\n",
    "    ax[1].set_title('')\n",
    "    ax[2].set_title('')\n",
    "    ax[3].set_title('')\n",
    "    ax[0].set_title('')\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[1].set_xlabel('')\n",
    "    ax[2].set_xlabel('')\n",
    "    ax[3].set_xlabel('E')\n",
    "    ax[1].set_ylabel('N')\n",
    "    ax[2].set_ylabel('N')\n",
    "    ax[3].set_ylabel('N')\n",
    "    ax[3].text(0.85, 0.72, f'RMSE:{rmse_map:.2f} m', transform=ax[3].transAxes, ha='left', va='top')\n",
    "    ax[3].text(0.85, 0.64, f'Mean:{mean_map:.2f} m', transform=ax[3].transAxes, ha='left', va='top')\n",
    "    ax[0].set_ylabel('Snow depth [m]')\n",
    "    #ax[0].tick_params(axis='x', which='both', bottom=False, top=True, labeltop=False)\n",
    "\n",
    "    # Combine legends for both y-axes\n",
    "    lines1, labels1 = ax[0].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax[0].legend(lines1 + lines2, labels1 + labels2, loc = 'upper left',framealpha=0.4)\n",
    "\n",
    "    # statistics\n",
    "    correlation, ksd, r2, _ = evaluate_difference(model.sel(y=n_profile, method='nearest'), dem_m.sel(y=n_profile, method='nearest'))\n",
    "    # aggregated \n",
    "    rmse = mean_squared_error(squeeze_N.dropna(dim='x'), np.zeros_like(squeeze_N.dropna(dim='x')),squared=False)\n",
    "    nmad = xdem.spatialstats.nmad(squeeze_N)\n",
    "    \n",
    "    print(f\"rmse of profile: {_:.2f}\")\n",
    "    \n",
    "    text_str = f\"Ï: {correlation:.2f}\\n$R^2$: {r2:.2f}\\nRMSE: {_:.2f} m\\nKSD: {ksd:.2f}\"\n",
    "    ax[0].text(0.85, 0.92, text_str, transform=ax[0].transAxes, ha='left', va='top')\n",
    "\n",
    "    print(f\"Spearman's Correlation: {correlation:.2f}\")\n",
    "    print(f\"R-squared: {r2:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.1) # Adjust this value as needed\n",
    "    plt.savefig('a_'+title, dpi=600, bbox_inches='tight')\n",
    "    return difference_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xsnow.goregression import quantile_mapping_xr\n",
    "import xdem \n",
    "\n",
    "als_path = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\nve_08_merge_m.tif' # Path to the DEM TIFF file\n",
    "als = rioxarray.open_rasterio(als_path)\n",
    "test = case_6.timeseries.sel(time='2008-04-01').rename({'E':'x','N':'y'})\n",
    "test.rio.set_crs(als.rio.crs)\n",
    "als = als.where(als != als.rio.nodata).squeeze()\n",
    "als_m = als.rio.reproject_match(test)\n",
    "\n",
    "# add correct value for test (xr.Dataset)\n",
    "with open('adjust_factor_08_case6.pkl', 'rb') as f:\n",
    "    adjust_factor = pickle.load(f)\n",
    "    for als, factor in adjust_factor.items():\n",
    "        #quantile_mapping_original_xr(test,factor['delta_gt1'],factor['dem_q_gt1'],dem=dem, split=0)\n",
    "        quantile_mapping_xr(test,factor['delta_lt1'],factor['delta_gt1'],dem=als, split=0)\n",
    "\n",
    "load_dem_plot_difference(als_m,test.sd_predict_dtm1_,test.z,xlim=(54000,61000),ylim=(6694752,6695306),title='snowdepth_profile_08_case6_corr.jpg');\n",
    "load_dem_plot_difference(als_m,test.sd_predict_dtm1 -1,test.z,xlim=(54000,61000),ylim=(6694752,6695306),title='snowdepth_profile_08_case6_original.jpg');#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dem_plot_difference(als_m,test.sd_predict_dtm1 -1,test.z,xlim=(54000,61000),ylim=(6694752,6695306),title='snowdepth_profile_08_case6_original.jpg');#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from xsnow.goregression import quantile_mapping_xr \n",
    "als_path = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\nve_09_merge_m.tif' # Path to the DEM TIFF file\n",
    "als = rioxarray.open_rasterio(als_path)\n",
    "test = case_6.timeseries.sel(time='2009-04-01').rename({'E':'x','N':'y'})\n",
    "test.rio.set_crs(als.rio.crs)\n",
    "als = als.where(als != als.rio.nodata).squeeze()\n",
    "als_m = als.rio.reproject_match(test)\n",
    "import xdem\n",
    "\n",
    "test = case_6.timeseries.sel(time='2009-04-01').rename({'E':'x','N':'y'})\n",
    "with open('adjust_factor_09_case6.pkl', 'rb') as f:\n",
    "    adjust_factor = pickle.load(f)\n",
    "    for dem, factor in adjust_factor.items():\n",
    "        #quantile_mapping_original_xr(test,factor['delta_gt1'],factor['dem_q_gt1'],dem=dem, split=0)\n",
    "        quantile_mapping_xr(test,factor['delta_lt1'],factor['delta_gt1'],dem=dem, split=0)#\n",
    "\n",
    "load_dem_plot_difference(als_m,test.sd_predict_dtm1_,test.z,xlim=(54000,61000),ylim=(6694752,6695306),title='snowdepth_profile_09_case6_corr.jpg');\n",
    "load_dem_plot_difference(als_m,test.sd_predict_dtm1 -1,test.z,xlim=(54000,61000),ylim=(6694752,6695306),title='snowdepth_profile_09_case6_original.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.1 There are snow pacthes in DTM1, validated by ALS bare ground data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_path = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\data\\Snow_depth_on_Hardangervidda_2008-2009\\Snow_depth_on_Hardangervidda_2008-2009\\FlightLine_2_DTM_200809.tif' # Path to the DEM TIFF file\n",
    "dtm = rioxarray.open_rasterio(dtm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.enums import Resampling\n",
    "from xsnow.goplot import normal_statistics\n",
    "from noisyopt import minimizeCompass\n",
    "\n",
    "# read\n",
    "dtm_ = dtm.rio.reproject_match(test).where(dtm_ >0)\n",
    "\n",
    "def get_dh_by_shift_px_xr(test,dtm_,shift_m,return_dh=False):\n",
    "    # shift\n",
    "    dtm_shift = dtm_.assign_coords({\"x\": (dtm_.x + shift_m[0]),'y': (dtm_.y + shift_m[1])})\n",
    "    # difference\n",
    "    difference_map = dtm_shift.rio.reproject_match(test, resampling = Resampling.bilinear) - test.z\n",
    "    # loss\n",
    "    res = {}\n",
    "    res['mean'],res['median'],res['std'],res['rmse'],res['num'],res['nmad'] = normal_statistics(difference_map , perc_t = 100, std_t = None)\n",
    "    if return_dh:\n",
    "        return difference_map\n",
    "\n",
    "    return res['nmad']\n",
    "\n",
    "\n",
    "#func_x = lambda x: get_dh_by_shift_px_xr(test,dtm_,x)\n",
    "#res = minimizeCompass(func_x, x0=(0,0), deltainit=5,deltatol=0.006,feps=0.0002,bounds=([-10,10],[-10,10]),disp=True,errorcontrol=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_map = get_dh_by_shift_px_xr(test,dtm_,(-2.8125,  3.125),return_dh=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "xlim=(54000,61000)\n",
    "ylim=(6694752,6695306)\n",
    "fig, ax = plt.subplots(1,1,figsize=(16, 2),dpi=600)\n",
    "difference_map.sel(x=slice(xlim[0],xlim[1]),y=slice(ylim[0],ylim[1])).plot(cmap='coolwarm_r', vmin=-4, vmax=4,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "case_m = Snow_Distributor(new_df,\n",
    "                          era,\n",
    "                          features=['E','N','h_te_best_fit','slope', 'aspect', \n",
    "                                  'planc','profc','tpi','tpi_9','tpi_27',\n",
    "                                  'curvature','sde_era','wf_positive', \n",
    "                                  'wf_negative','month'],\n",
    "                          regressor_list = ['sd_dtm1_abserror_250_10_qc_nve_geosegment_abs.json',\n",
    "                                             'sd_dtm10_abserror_250_10_qc_nve_geosegment_abs.json',\n",
    "                                             'sd_cop30_abserror_250_10_qc_nve_geosegment_abs.json',\n",
    "                                             'sd_fab_abserror_250_10_qc_nve_geosegment_abs.json'],\n",
    "                          crop=[6649500, 6670000, 98961, 123200])\n",
    "\n",
    "case_m.product_time_series(start_date = pd.to_datetime('20150101'), correction='qm',regression='MAD');\n",
    "case_m.timeseries = case_m.mask_out_lake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datashader.transfer_functions import shade\n",
    "from datashader.transfer_functions import stack\n",
    "import datashader.transfer_functions as tf\n",
    "import matplotlib.cm as cm\n",
    "import holoviews as hv, datashader as ds, geoviews as gv, geoviews.tile_sources as gvts\n",
    "import imageio\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_snow_depth_gif_3d(ds_timeseries,variable = 'sd_predict_dtm1_',cmap=cm.get_cmap('Spectral'),offset=0):\n",
    "\n",
    "        # Assuming you have an xarray Dataset named 'ds' with dimensions 'N', 'E', and 'time'\n",
    "\n",
    "        # Create an empty list to store the images\n",
    "        images = []\n",
    "\n",
    "        terrain = ds_timeseries.z\n",
    "        illuminated = hillshade(terrain)\n",
    "\n",
    "        # Create a custom colormap by modifying the original colormap\n",
    "        custom_cmap = cmap\n",
    "        #custom_cmap = mcm.get_cmap(cmap, 256)\n",
    "        #custom_cmap_colors = custom_cmap(np.linspace(0, 1, 256))\n",
    "        #custom_cmap_colors[:int(0.055 * 256)] = [1, 1, 1, 1]  # Set values below 0.1 to white\n",
    "        #custom_cmap = ListedColormap(custom_cmap_colors)\n",
    "\n",
    "        # Iterate over the time dimension\n",
    "        for t in ds_timeseries.time:\n",
    "            # Extract the data for the specific time slice\n",
    "            data_slice = ds_timeseries[variable].sel(time=t) - offset\n",
    "  \n",
    "            # Create a map plot\n",
    "\n",
    "            # Plot the terrain, illuminated layer, and snow depth using stack\n",
    "            img = tf.stack(\n",
    "                shade(terrain, cmap=[\"black\", \"white\"], how=\"linear\"),\n",
    "                shade(illuminated, cmap=[\"black\", \"white\"], how=\"linear\", alpha=150),\n",
    "                shade(data_slice, cmap=custom_cmap, how=\"linear\", alpha=180, span=[0, 2])\n",
    "            )\n",
    "\n",
    "            # Format the time for the plot title\n",
    "            t_formatted = t.dt.strftime('%Y-%m').item()\n",
    "\n",
    "            # Save the figure as an image\n",
    "            filename = f\"{t_formatted}.jpg\"\n",
    "   \n",
    "            ds.utils.export_image(img, t_formatted, background='white', export_path='.')\n",
    "\n",
    "            # Append the image to the list\n",
    "            images.append(imageio.imread(filename))\n",
    "\n",
    "        # Save the list of images as a GIF\n",
    "        imageio.mimsave('animation_3d_.gif', images, duration=0.8)\n",
    "\n",
    "def plot_snow_depth_gif_chart(ds_timeseries, variable='sde_predict_dtm1',offset=0):\n",
    "        # Create an empty list to store the images\n",
    "        images = []\n",
    "\n",
    "\n",
    "        # Plot scatter for each time snapshot\n",
    "        for t in ds_timeseries.time:\n",
    "            # Plot chart for mean value\n",
    "            fig, ax = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "            mean_series = ds_timeseries[variable].mean(dim=['N', 'E']) - offset\n",
    "            mean_series.plot(x='time', ax=ax)\n",
    "            ax.set_ylabel('Mean Snow depth (m)')\n",
    "\n",
    "            plt.scatter(t, mean_series.sel(time=t), c='red', marker='o')\n",
    "\n",
    "            # Format the time for the plot title\n",
    "            t_formatted = t.dt.strftime('%Y-%m').item()\n",
    "            ax.set_title(f'Monthly Snow Depth - {t_formatted}',fontsize=20)\n",
    "\n",
    "            # Save the figure as an image\n",
    "            filename = f\"{t_formatted}.jpg\"\n",
    "            plt.savefig(filename, dpi=100)\n",
    "            plt.close(fig)\n",
    "\n",
    "            # Append the image to the list\n",
    "            images.append(imageio.imread(filename))\n",
    "\n",
    "        # Save the list of images as a GIF\n",
    "        imageio.mimsave('animation_chart.gif', images, duration=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_snow_depth_gif_3d(case_m.timeseries.sel(N=slice(6649500, 6670000), E=slice(98961, 123200)),variable = 'sd_predict_dtm1',offset=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_snow_depth_gif_chart(case_m.timeseries.sel(N=slice(6657480, 6657520), E=slice(105000, 105040)),variable='sd_predict_dtm1',offset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Weather station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read frost data\n",
    "\n",
    "from xsnow.read_frost import find_station,read_frost_loop\n",
    "\n",
    "station_list = find_station(country=None,polygon=None,sources='SN46510,SN33990,SN33950,SN33890,SN31680,SN29400,SN31660')\n",
    "station_list[station_list['id'] == 'SN33950'][['E', 'N']]\n",
    "station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_measurements = read_frost_loop(sources=['SN33950','SN29400','SN31660'], #'SN33990','SN46510','SN33890'\n",
    "                          elements=['mean(surface_snow_thickness P1M)','max(surface_snow_thickness P1M)','min(surface_snow_thickness P1M)'],\n",
    "                          referencetime='2008-01-01/2022-12-01')\n",
    "\n",
    "#station_d_snow = read_frost_loop(sources=['SN46510','SN33990','SN33950','SN33890'],\n",
    "#                          elements=['max(surface_snow_thickness P1M)'],\n",
    "#                          referencetime='2015-01-01/2022-12-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "import xsnow.misc\n",
    "reload(xsnow.misc)\n",
    "from xsnow.misc import temporal_statistic\n",
    "\n",
    "def compare_with_station(station_m_snow,\n",
    "                         station_list,\n",
    "                         case,\n",
    "                         station='SN33950',\n",
    "                         nn=None,\n",
    "                         ee=None,\n",
    "                         name=None,\n",
    "                         offset=0,\n",
    "                         dem='sd_predict_dtm1_',\n",
    "                         ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(1,2,figsize=(16,3), gridspec_kw={'width_ratios': [4,1]},dpi=600)\n",
    "\n",
    "    s_mean = station_m_snow[station_m_snow['elementId']=='mean(surface_snow_thickness P1M)']['value'] /100\n",
    "    s_min = station_m_snow[station_m_snow['elementId']=='max(surface_snow_thickness P1M)']['value'] /100\n",
    "    s_max= station_m_snow[station_m_snow['elementId']=='min(surface_snow_thickness P1M)']['value'] /100\n",
    "\n",
    "    # plot station\n",
    "    s_mean.plot(ax=ax[0],label='Weather station')\n",
    "    ax[0].fill_between(s_mean.index, s_min, s_max, color='blue', alpha=0.15)\n",
    "\n",
    "    # plot model\n",
    "    if nn is None:\n",
    "        nn = station_list[station_list['id'] == station]['N'].values[0]\n",
    "    \n",
    "    if ee is None:\n",
    "        ee = station_list[station_list['id'] == station]['E'].values[0]\n",
    "    \n",
    "    # sd_predict_dtm1 or sd_predict_dtm1_                    \n",
    "    model_sd = case.timeseries.sel(N=slice(nn-10,nn+10),E=slice(ee-10,ee+10)).mean(dim=['N','E'])\n",
    "    \n",
    "    # adjust offset and clip negative prediction.\n",
    "    model_sd[dem] = (model_sd[dem]- offset).clip(min=0)\n",
    "    model_sd[dem].plot(ax=ax[0],label='Downscaled output (10 m)',alpha=0.8)\n",
    "    model_sd.sde_era.plot(ax=ax[0],label='ERA5 Land',color='red',alpha=0.5,linestyle='--')\n",
    "    \n",
    "    if f'{dem}_25' in model_sd:\n",
    "        model_sd[f'{dem}_25'] = (model_sd[f'{dem}_25']- offset).clip(min=0)\n",
    "        model_sd[f'{dem}_75'] = (model_sd[f'{dem}_75']- offset).clip(min=0)\n",
    "        ax[0].fill_between(model_sd[dem].time, model_sd[f'{dem}_25'], model_sd[f'{dem}_75'], color='yellow', alpha=0.25)\n",
    "\n",
    "    # plot the map\n",
    "    sd_to_plot = case.timeseries[dem].mean(dim=['time']) - offset\n",
    "    map_label = \"Average snow depth [m]\"\n",
    "    if f'{dem}_25' in model_sd:\n",
    "            sd_to_plot = (case.timeseries[f'{dem}_75'] - case.timeseries[f'{dem}_25']).mean(dim=['time'])\n",
    "            map_label = 'Snow depth IQR [m]'\n",
    "    sd_to_plot.sel(N=slice(nn-250,nn+250),E=slice(ee-250,ee+250)).plot(ax=ax[1],cmap='Spectral', vmin=0, vmax=model_sd.sde_era.max(), cbar_kwargs={\"label\": map_label})\n",
    "    ax[1].scatter(ee, nn, color='black', marker='o',facecolors='none')\n",
    "    ax[1].annotate(name, (ee, nn), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    # Add RMSE and R-squared values to the plot\n",
    "    dict_output = temporal_statistic(model_sd[dem].values.flatten(), s_mean.values,model_sd.time.values,s_mean.index.values)\n",
    "    text_str = f\"RMSE: {dict_output['rmse']:.2f} m\\n$R^2$: {dict_output['r2']:.2f}\\nÏ: {dict_output['spearman_r']:.2f}\\n$KSD$: {dict_output['ks']:.2f}\"\n",
    "\n",
    "    formatted_output = {key: f'{value:.3f}' for key, value in dict_output.items() if key in ['N','rmse','r2','spearman_r','ks']}\n",
    "    print(f'Output:\\n{formatted_output}')\n",
    "    \n",
    "    # Era5 land metrics\n",
    "    dict_era5 = temporal_statistic(model_sd.sde_era.values.flatten(), s_mean.values,model_sd.time.values,s_mean.index.values)\n",
    "    formatted_output_era = {key: f'{value:.3f}' for key, value in dict_era5.items() if key in ['N','rmse','r2','spearman_r','ks']}\n",
    "    print(f'ERA5 land:\\n{formatted_output_era}')\n",
    "    \n",
    "    ax[0].text(0.85, 0.95, text_str, transform=ax[0].transAxes, ha='left', va='top')\n",
    "    ax[0].set_ylabel('Snow depth [m]')\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].tick_params(axis='x', which='minor', bottom=False)\n",
    "    \n",
    "\n",
    "    ax[0].legend()\n",
    "    #ax[0].set_title(name)\n",
    "\n",
    "    return dict_output, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame called station_list\n",
    "stations = pd.DataFrame({\n",
    "    'id': ['SN31660', 'SN31661', 'SN31662'],\n",
    "    'N': [station_list[station_list['id'] == 'SN29400']['N'].values[0], \n",
    "          station_list[station_list['id'] == 'SN31660']['N'].values[0],\n",
    "          station_list[station_list['id'] == 'SN33950']['N'].values[0]],\n",
    "    'E': [station_list[station_list['id'] == 'SN29400']['E'].values[0], \n",
    "          station_list[station_list['id'] == 'SN31660']['E'].values[0],\n",
    "          station_list[station_list['id'] == 'SN33950']['E'].values[0]],\n",
    "})\n",
    "\n",
    "# Define the buffer size\n",
    "buffer_size = 250\n",
    "\n",
    "# Create a list to store subsets for each station\n",
    "subsets = []\n",
    "\n",
    "# Iterate through each station\n",
    "for station_id, station_data in stations.iterrows():\n",
    "    # Get the station's N and E coordinates\n",
    "    station_n = station_data['N']\n",
    "    station_e = station_data['E']\n",
    "    \n",
    "    # Create a subset by filtering based on coordinates\n",
    "    subset = new_df[\n",
    "        (new_df['N'] >= station_n - buffer_size) &\n",
    "        (new_df['N'] <= station_n + buffer_size) &\n",
    "        (new_df['E'] >= station_e - buffer_size) &\n",
    "        (new_df['E'] <= station_e + buffer_size)\n",
    "    ].copy()  # Add .copy() to create an independent copy\n",
    "    \n",
    "    # Append the subset to the list\n",
    "    subsets.append(subset)\n",
    "\n",
    "# Concatenate the subsets into a single DataFrame\n",
    "df_station = pd.concat(subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import xsnow.goregression\n",
    "import xsnow.read_frost\n",
    "reload(xsnow.goregression)\n",
    "reload(xsnow.read_frost)\n",
    "from xsnow.goregression import Snow_Distributor\n",
    "import pandas as pd\n",
    "\n",
    "case_station_6 = Snow_Distributor(df_station,era,\n",
    "                                features=['E', 'N','h_te_best_fit','slope', 'aspect', \n",
    "                                'planc','profc','tpi','tpi_9','tpi_27',\n",
    "                                'curvature','sde_era','wf_positive', \n",
    "                                'wf_negative','month'])\n",
    "\n",
    "case_station_6.regressor_list = ['sd_dtm10_abserror_250_10_qc_nve_geosegment_square.json',\n",
    "                                'sd_dtm1_abserror_250_10_qc_nve_geosegment_square.json',\n",
    "                                'sd_cop30_abserror_250_10_qc_nve_geosegment_square.json',\n",
    "                                'sd_fab_abserror_250_10_qc_nve_geosegment_square.json']\n",
    "\n",
    "# generate time series\n",
    "case_station_6.product_time_series(correction='qm',regression='other')\n",
    "case_station_6.timeseries = case_station_6.mask_out_lake(is_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import xsnow.goregression\n",
    "import xsnow.read_frost\n",
    "reload(xsnow.goregression)\n",
    "reload(xsnow.read_frost)\n",
    "from xsnow.goregression import Snow_Distributor\n",
    "import pandas as pd\n",
    "\n",
    "case_station_5 = Snow_Distributor(df_station,era,\n",
    "                                features=['E', 'N','h_te_best_fit','slope', 'aspect', \n",
    "                                'planc','profc','tpi','tpi_9','tpi_27',\n",
    "                                'curvature','sde_era','wf_positive', \n",
    "                                'wf_negative','month'])\n",
    "\n",
    "case_station_5.regressor_list = ['sd_dtm10_abserror_250_10_qc_nve_quantile_3.json',\n",
    "                                'sd_dtm1_abserror_250_10_qc_nve_quantile_3.json',\n",
    "                                'sd_cop30_abserror_250_10_qc_nve_quantile_3.json',\n",
    "                                'sd_fab_abserror_250_10_qc_nve_quantile_3.json']\n",
    "\n",
    "# generate time series\n",
    "case_station_5.product_time_series(correction='qm',regression='quantile_regression')\n",
    "case_station_5.timeseries = case_station_5.mask_out_lake(is_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def plot_station_timeseries(case_s,station_list,dem='sd_predict_dtm1_',offset=0,title='case.jpg'):\n",
    "\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(16, 11), gridspec_kw={'width_ratios': [4, 1]}, dpi=600, sharex=False)\n",
    "    \n",
    "    # Disable sharing of x-axis for the subplots in the second column\n",
    "    for i in range(3):\n",
    "        ax[i, 0].sharex(ax[0, 0])\n",
    "\n",
    "    metrics_a, ax_a = compare_with_station(station_measurements[station_measurements['station_id'] == 'SN29400'],\n",
    "                        station_list,\n",
    "                        case_s,\n",
    "                        station='SN29400',\n",
    "                        name='Sandhaug',\n",
    "                        nn=station_list[station_list['id'] == 'SN29400']['N'].values[0],\n",
    "                        ee=station_list[station_list['id'] == 'SN29400']['E'].values[0],\n",
    "                        dem=dem,\n",
    "                        offset=offset,\n",
    "                        ax=(ax[0,0],ax[0,1]))\n",
    "\n",
    "    metrics_b, ax_b = compare_with_station(station_measurements[station_measurements['station_id'] == 'SN31660'],\n",
    "                        station_list,\n",
    "                        case_s,\n",
    "                        station='SN31660',\n",
    "                        name='Mogen',\n",
    "                        nn=station_list[station_list['id'] == 'SN31660']['N'].values[0],\n",
    "                        ee=station_list[station_list['id'] == 'SN31660']['E'].values[0],\n",
    "                        dem=dem,\n",
    "                        offset=offset,\n",
    "                        ax=(ax[1,0],ax[1,1]))\n",
    "\n",
    "    metrics_c, ax_c = compare_with_station(station_measurements[station_measurements['station_id'] == 'SN33950'],\n",
    "                        station_list,\n",
    "                        case_s,\n",
    "                        station='SN33950',\n",
    "                        name='Haukeliseter',\n",
    "                        nn=station_list[station_list['id'] == 'SN33950']['N'].values[0],\n",
    "                        ee=station_list[station_list['id'] == 'SN33950']['E'].values[0],\n",
    "                        dem=dem,\n",
    "                        offset=offset,\n",
    "                        ax=(ax[2,0],ax[2,1]))\n",
    "\n",
    "    ax[2,0].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax[2,0].xaxis.set_major_locator(mdates.YearLocator())\n",
    "    \n",
    "    fig_s, ax_s = plt.subplots()\n",
    "    # Scatter plot for the first DataFrame (group 1)\n",
    "    ax_s.scatter(metrics_a['df_2'], metrics_a['df_1'], label='Sandhaug', c='aqua', marker='.')\n",
    "    ax_s.scatter(metrics_b['df_2'], metrics_b['df_1'], label='Mogen', c='salmon', marker='.')\n",
    "    ax_s.scatter(metrics_c['df_2'], metrics_c['df_1'], label='Haukeliseter', c='olive', marker='.')\n",
    "\n",
    "    # Perform linear regression for the combined data\n",
    "    x = pd.concat([metrics_a['df_2'], metrics_b['df_2'], metrics_c['df_2']], ignore_index=True)\n",
    "    y = pd.concat([metrics_a['df_1'], metrics_b['df_1'], metrics_c['df_1']], ignore_index=True)\n",
    "    \n",
    "    # Calculate the regression line \n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x[0], y[0])\n",
    "    \n",
    "    # Create the regression line\n",
    "    regression_line = slope * x + intercept\n",
    "    # Plot the regression line\n",
    "    ax_s.plot(x, regression_line, label=f'y={slope:.2f}x+{intercept:.2f}', linestyle='--', color='red')\n",
    "    # Add R-squared and N to the plot\n",
    "\n",
    "    ax_s.text(0.8, 0.9, f'RÂ² = {r_value**2:.2f}', transform=ax_s.transAxes, fontsize=9, color='black')\n",
    "    ax_s.text(0.8, 0.85, f'N = {len(x)}', transform=ax_s.transAxes, fontsize=9, color='black')\n",
    "\n",
    "    # Set the axis limits\n",
    "    ax_s.set_xlim([0, 2.5])\n",
    "    ax_s.set_ylim([0, 2.5])\n",
    "    # Add labels and legend\n",
    "    ax_s.set_xlabel('Station measurements [m]')\n",
    "    ax_s.set_ylabel('Downscaled output [m]')\n",
    "    plt.legend()\n",
    "    fig_s.savefig(f'weather_scatter_{title}',dpi=600, format='jpg')\n",
    "    #ax_a.savefig(f'weather_s_{title}', dpi=600, format='jpg')\n",
    "    #ax_b.savefig(f'weather_m_{title}',dpi=600, format='jpg')\n",
    "    #ax_c.savefig(f'weather_h_{title}',dpi=600, format='jpg')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_station_timeseries(case_station_5,station_list,dem='sd_predict_dtm10',offset=1,title='case5_original_10.jpg ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_station_timeseries(case_station_6,station_list,dem='sd_predict_fab',offset=1,title='case6_fab.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_station_timeseries(case_station_60,station_list,dem='sd_predict_dtm1',offset=0,title='case6.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile regression with Q05 ADN Q95, you can change it to Q25 AND Q75\n",
    "plot_station_timeseries(case_station_5,station_list,dem='sd_predict_dtm1_75',offset=1,title='case5_75.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Variogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xdem\n",
    "\n",
    "condition = (als_m > 0) & (mask.rename({'E':'x','N':'y'}) == 0)\n",
    "\n",
    "(\n",
    "    df_empirical_variogram,\n",
    "    df_model_params,\n",
    "    spatial_corr_function,\n",
    ") = xdem.spatialstats.infer_spatial_correlation_from_stable(\n",
    "    dvalues=als_m.to_numpy(),\n",
    "    stable_mask=condition.to_numpy(),\n",
    "    list_models=[\"spherical\",'gaussian'],\n",
    "    subsample=10000,\n",
    "    gsd=10,\n",
    "    random_state=42,\n",
    "    n_variograms=10,\n",
    "    n_jobs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = case_6.timeseries.sel(time='2008-04-01').rename({'E':'x','N':'y'})\n",
    "\n",
    "from xsnow.goregression import quantile_mapping_original_xr\n",
    "import pickle\n",
    "\n",
    "# add correct value for test (xr.Dataset)\n",
    "with open('adjust_factor_08_case6.pkl', 'rb') as f:\n",
    "    adjust_factor = pickle.load(f)\n",
    "    for als, factor in adjust_factor.items():\n",
    "        quantile_mapping_original_xr(test, factor['delta_gt1'],factor['dem_q_gt1'],dem=als, split=0)\n",
    "\n",
    "(\n",
    "    df_empirical_variogram_dtm_,\n",
    "    df_model_params_dtm_,\n",
    "    spatial_corr_function_dtm_,\n",
    ") = xdem.spatialstats.infer_spatial_correlation_from_stable(\n",
    "    dvalues=test.sd_predict_dtm1.to_numpy(),\n",
    "    stable_mask=condition.to_numpy(),\n",
    "    list_models=[\"spherical\",'gaussian'],\n",
    "    subsample=10000,\n",
    "    gsd=10,\n",
    "    random_state=42,\n",
    "    n_variograms=10,\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_empirical_variogram_se,\n",
    "    df_model_params_se,\n",
    "    spatial_corr_function_se,\n",
    ") = xdem.spatialstats.infer_spatial_correlation_from_stable(\n",
    "    dvalues=test.sde_se.to_numpy(),\n",
    "    stable_mask=condition.to_numpy(),\n",
    "    list_models=[\"spherical\",'gaussian'],\n",
    "    subsample=10000,\n",
    "    gsd=10,\n",
    "    random_state=42,\n",
    "    n_variograms=10,\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    df_empirical_variogram_dtm10_,\n",
    "    df_model_params_dtm10_,\n",
    "    spatial_corr_function_dtm10_,\n",
    ") = xdem.spatialstats.infer_spatial_correlation_from_stable(\n",
    "    dvalues=test.sd_predict_dtm10.to_numpy(),\n",
    "    stable_mask=condition.to_numpy(),\n",
    "    list_models=[\"spherical\",'gaussian'],\n",
    "    subsample=10000,\n",
    "    gsd=10,\n",
    "    random_state=42,\n",
    "    n_variograms=10,\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_empirical_variogram_cop30_,\n",
    "    df_model_params_cop30_,\n",
    "    spatial_corr_function_cop30_,\n",
    ") = xdem.spatialstats.infer_spatial_correlation_from_stable(\n",
    "    dvalues=test.sd_predict_cop30.to_numpy(),\n",
    "    stable_mask=condition.to_numpy(),\n",
    "    list_models=[\"spherical\",'gaussian'],\n",
    "    subsample=10000,\n",
    "    gsd=10,\n",
    "    random_state=42,\n",
    "    n_variograms=10,\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_empirical_variogram_fab_,\n",
    "    df_model_params_fab_,\n",
    "    spatial_corr_function_fab_,\n",
    ") = xdem.spatialstats.infer_spatial_correlation_from_stable(\n",
    "    dvalues=test.sd_predict_fab.to_numpy(),\n",
    "    stable_mask=condition.to_numpy(),\n",
    "    list_models=[\"spherical\",'gaussian'],\n",
    "    subsample=10000,\n",
    "    gsd=10,\n",
    "    random_state=42,\n",
    "    n_variograms=10,\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_variogram_s(\n",
    "    df_list: list[pd.DataFrame],\n",
    "    list_fit_fun: list[list[Callable[[np.ndarray], np.ndarray]]] = None,\n",
    "    list_fit_fun_label: list[str] = None,\n",
    "    ax: matplotlib.axes.Axes = None,\n",
    "    xscale: str = \"linear\",\n",
    "    xscale_range_split: list[float] = None,\n",
    "    labels: list[str] = None,\n",
    "    xlabel: str = None,\n",
    "    ylabel: str = None,\n",
    "    xlim: str = None,\n",
    "    ylim: str = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot empirical variogram, and optionally also plot one or several model fits.\n",
    "    Input dataframe is expected to be the output of xdem.spatialstats.sample_empirical_variogram.\n",
    "    Input function model is expected to be the output of xdem.spatialstats.fit_sum_model_variogram.\n",
    "\n",
    "    :param df: Empirical variogram, formatted as a dataframe with count (pairwise sample count), lags\n",
    "        (upper bound of spatial lag bin), exp (experimental variance), and err_exp (error on experimental variance)\n",
    "    :param list_fit_fun: List of model function fits\n",
    "    :param list_fit_fun_label: List of model function fits labels\n",
    "    :param ax: Plotting ax to use, creates a new one by default\n",
    "    :param xscale: Scale of X-axis\n",
    "    :param xscale_range_split: List of ranges at which to split the figure\n",
    "    :param xlabel: Label of X-axis\n",
    "    :param ylabel: Label of Y-axis\n",
    "    :param xlim: Limits of X-axis\n",
    "    :param ylim: Limits of Y-axis\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    color = [\"#1f77b4\", # Strong blue for R1\n",
    "            \"#ff7f0e\", # Strong orange for A1\n",
    "            \"#e7ba52\", # Lighter orange for A2\n",
    "            \"#2ca02c\", # Strong green for B1\n",
    "            \"#17becf\", # Lighter blue for B2\n",
    "            \"#b3b3b3\"  # Muted grey for R2\n",
    "            ]\n",
    "\n",
    "    # Create a dark palette\n",
    "    #color = sns.color_palette(\"bright\")  # Starting with a dark red\n",
    "\n",
    "    # Create axes if they are not passed\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "    elif isinstance(ax, matplotlib.axes.Axes):\n",
    "        fig = ax.figure\n",
    "    else:\n",
    "        raise ValueError(\"ax must be a matplotlib.axes.Axes instance or None\")\n",
    "\n",
    "    # Check format of input dataframe\n",
    "    expected_values = [\"exp\", \"lags\", \"count\"]\n",
    "    for val in expected_values:\n",
    "        if val not in df_list[0].columns.values:\n",
    "            raise ValueError(f'The expected variable \"{val}\" is not part of the provided dataframe column names.')\n",
    "\n",
    "    # Hide axes for the main subplot (which will be subdivded)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    if ylabel is None:\n",
    "        ylabel = r\"Variance [$\\mu$ $\\pm \\sigma$]\"\n",
    "    if xlabel is None:\n",
    "        xlabel = \"Spatial lag (m)\"\n",
    "\n",
    "    init_gridsize = [10, 10]\n",
    "    # Create parameters to split x axis into different linear scales\n",
    "    # If there is no split, get parameters for a single subplot\n",
    "    if xscale_range_split is None:\n",
    "        nb_subpanels = 1\n",
    "        if xscale == \"log\":\n",
    "            xmin = [np.min(df_list[0].lags) / 2]\n",
    "        else:\n",
    "            xmin = [0]\n",
    "        xmax = [np.max(df_list[0].lags)]\n",
    "        xgridmin = [0]\n",
    "        xgridmax = [init_gridsize[0]]\n",
    "        gridsize = init_gridsize\n",
    "    # Otherwise, derive a list for each subplot\n",
    "    else:\n",
    "        # Add initial zero if not in input\n",
    "        if xscale_range_split[0] != 0:\n",
    "            if xscale == \"log\":\n",
    "                first_xmin = np.min(df_list[0].lags) / 2\n",
    "            else:\n",
    "                first_xmin = 0\n",
    "            xscale_range_split = [first_xmin] + xscale_range_split\n",
    "        # Add maximum distance if not in input\n",
    "        if xscale_range_split[-1] != np.max(df_list[0].lags):\n",
    "            xscale_range_split.append(np.max(df_list[0].lags))\n",
    "\n",
    "        # Scale grid size by the number of subpanels\n",
    "        nb_subpanels = len(xscale_range_split) - 1\n",
    "        gridsize = init_gridsize.copy()\n",
    "        gridsize[0] *= nb_subpanels\n",
    "        # Create list of parameters to pass to ax/grid objects of subpanels\n",
    "        xmin = []\n",
    "        xmax = []\n",
    "        xgridmin = []\n",
    "        xgridmax = []\n",
    "        for i in range(nb_subpanels):\n",
    "            xmin.append(xscale_range_split[i])\n",
    "            xmax.append(xscale_range_split[i + 1])\n",
    "            xgridmin.append(init_gridsize[0] * i)\n",
    "            xgridmax.append(init_gridsize[0] * (i + 1))\n",
    "\n",
    "    # Need a grid plot to show the sample count and the statistic\n",
    "    grid = plt.GridSpec(gridsize[1], gridsize[0], wspace=0.5, hspace=0.5)\n",
    "\n",
    "    # Loop over each subpaneld\n",
    "    for k in range(nb_subpanels):\n",
    "        # First, an axis to plot the sample histogram\n",
    "        ax0 = ax.inset_axes(grid[:2, xgridmin[k] : xgridmax[k]].get_position(fig).bounds)\n",
    "        ax0.set_xscale(xscale)\n",
    "        ax0.set_xticks([])\n",
    "\n",
    "        # Plot the histogram manually with fill_between\n",
    "        interval_var = [0] + list(df_list[0].lags)\n",
    "        for i in range(len(df_list[0])):\n",
    "            count = df_list[0][\"count\"].values[i]\n",
    "            ax0.fill_between(\n",
    "                [interval_var[i], interval_var[i + 1]],\n",
    "                [0] * 2,\n",
    "                [count] * 2,\n",
    "                facecolor=plt.cm.Greys(0.65),\n",
    "                alpha=1,\n",
    "                edgecolor=\"white\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "        if k == 0:\n",
    "            ax0.set_ylabel(\"Sample count\")\n",
    "            ax0.set_yscale('log')\n",
    "            # Scientific format to avoid undesired additional space on the label side\n",
    "            #ax0.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "        else:\n",
    "            ax0.set_yscale('log')\n",
    "            ax0.set_yticklabels([])\n",
    "            ax0.set_yticks([])\n",
    "            ax0.yaxis.set_ticks_position('none') \n",
    "\n",
    "        # Ignore warnings for log scales\n",
    "        ax0.set_xlim((xmin[k], xmax[k]))\n",
    "        ax0.grid(False)\n",
    "        \n",
    "        # Now, plot the statistic of the data\n",
    "        ax1 = ax.inset_axes(grid[2:, xgridmin[k] : xgridmax[k]].get_position(fig).bounds)\n",
    "\n",
    "        # Get the lags bin centers\n",
    "        bins_center = np.subtract(df_list[0].lags, np.diff([0] + df_list[0].lags.tolist()) / 2)\n",
    "\n",
    "        # If all the estimated errors are all NaN (single run), simply plot the empirical variogram\n",
    "        for i, df in enumerate(df_list):\n",
    "            if np.all(np.isnan(df.err_exp)):\n",
    "                ax1.scatter(bins_center, df['exp'], label=labels[0], color=color[i], marker=\"x\")\n",
    "            # Otherwise, plot the error estimates through multiple runs\n",
    "            else:\n",
    "                ax1.errorbar(bins_center, df['exp'], yerr=df['err_exp'], color=color[i], label=labels[0], fmt=\"x\")\n",
    "                \n",
    "        ax1.errorbar([], [], [], color='black', label='Empirical variogram (1-sigma std error)', fmt='x')\n",
    "        \n",
    "        # If a list of functions is passed, plot the modelled variograms\n",
    "        if list_fit_fun is not None:\n",
    "            for i, func in enumerate(list_fit_fun):\n",
    "                for fit_fun in func:\n",
    "                    x = np.linspace(xmin[k], xmax[k], 1000)\n",
    "                    y = fit_fun(x)\n",
    "                    ax1.plot(x, y, linestyle=\"dashed\", color=color[i], label=list_fit_fun_label[i], zorder=30)\n",
    "\n",
    "        ax1.set_xscale(xscale)\n",
    "        if nb_subpanels > 1 and k == (nb_subpanels - 1):\n",
    "            ax1.xaxis.set_ticks(np.linspace(xmin[k], xmax[k], 3))\n",
    "        elif nb_subpanels > 1:\n",
    "            ax1.xaxis.set_ticks(np.linspace(xmin[k], xmax[k], 3)[:-1])\n",
    "\n",
    "        if xlim is None:\n",
    "            ax1.set_xlim((xmin[k], xmax[k]))\n",
    "        else:\n",
    "            ax1.set_xlim(xlim)\n",
    "\n",
    "        if ylim is not None:\n",
    "            ax1.set_ylim(ylim)\n",
    "        else:\n",
    "            ax1.set_ylim((0, 1.05 * np.nanmax(df_list[0].exp)))\n",
    "\n",
    "        if k == 1:\n",
    "            ax1.set_xlabel(xlabel, x=1, ha='center')\n",
    "        if k == nb_subpanels - 1:\n",
    "            ax1.legend(loc=\"lower right\")\n",
    "        if k == 0:\n",
    "            ax1.set_ylabel(ylabel)\n",
    "        else:\n",
    "            ax1.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_s,ax_s = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# plot\n",
    "plot_variogram_s(\n",
    "    df_list=[df_empirical_variogram,\n",
    "             df_empirical_variogram_dtm_,\n",
    "             df_empirical_variogram_dtm10_,\n",
    "             df_empirical_variogram_cop30_,\n",
    "             df_empirical_variogram_fab_,\n",
    "             df_empirical_variogram_se],\n",
    "    list_fit_fun=[[xdem.spatialstats.get_variogram_model_func(df_model_params)],\n",
    "                  [xdem.spatialstats.get_variogram_model_func(df_model_params_dtm_)],\n",
    "                  [xdem.spatialstats.get_variogram_model_func(df_model_params_dtm10_)],\n",
    "                  [xdem.spatialstats.get_variogram_model_func(df_model_params_cop30_)],\n",
    "                  [xdem.spatialstats.get_variogram_model_func(df_model_params_fab_)],\n",
    "                  [xdem.spatialstats.get_variogram_model_func(df_model_params_se)]],\n",
    "    list_fit_fun_label=[\"Model fit - ALS\", \n",
    "                        \"Model fit - Downscaling (DTM1)\",\n",
    "                        'Model fit - Downscaling (DTM10)',\n",
    "                        'Model fit - Downscaling (COP30)',\n",
    "                        'Model fit - Downscaling (FAB)',\n",
    "                        'Model fit - seNorge'],  # Add appropriate labels for your models\n",
    "    labels=[\"\",'','','',''],\n",
    "    xlabel=\"Spatial lag (m)\",\n",
    "    ylabel=\"Semi variance ($m^2$)\",\n",
    "    xscale_range_split=[120, 640, 3600],\n",
    "    ax = ax_s\n",
    ")\n",
    "\n",
    "plt.savefig('variogram.jpg',dpi=600)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
