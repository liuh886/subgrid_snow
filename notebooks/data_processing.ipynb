{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Co-registration\n",
    "\n",
    "(1) We processed DEMs co-registration tile by tile, and get all results in .csv, which you can access from [Zenodo](https://zenodo.org/records/10048875).\n",
    "\n",
    "(2) The alignment with ERA5 Land is done in separated notebook.\n",
    "\n",
    "Here we start from Step 2 bias correction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Bias correction\n",
    "\n",
    "This step includes the regression and the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# all function has been moduled in to xsnow\n",
    "from xsnow.goregression import Snow_Regressor\n",
    "\n",
    "# load the dataset. You might need to change the path/file name\n",
    "sf = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\snow_free_.csv'\n",
    "sc = r'\\\\hypatia.uio.no\\lh-mn-geofag-felles\\projects\\snowdepth\\zhihaol\\snow_cover_.csv'\n",
    "dems = Snow_Regressor(sf,sc)\n",
    "\n",
    "# basic configuration\n",
    "dems.sc['flag'] = 'sc'\n",
    "dems.sf['flag'] = 'sf'\n",
    "dems.era = 'sde_era'\n",
    "\n",
    "# Using the geosegment\n",
    "dems.use_geosegment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start regression\n",
    "\n",
    "# setting the features for regression\n",
    "v_x_dtm1=['cloud_flag_atm','subset_te_flag', 'N', 'E','h_te_std','h_te_skew','terrain_slope',\n",
    "          'coreg_bias_dtm1',\n",
    "          'h_te_best_fit','pair','beam','slope', 'aspect', 'planc','profc','tpi','tpi_9','tpi_27','curvature',\n",
    "          'urban_flag', 'night_flag','region','difference']\n",
    "\n",
    "v_x_dtm10=['cloud_flag_atm','subset_te_flag', 'N', 'E','h_te_std','h_te_skew','terrain_slope',\n",
    "          'coreg_bias_dtm10',\n",
    "          'h_te_best_fit','pair','beam','slope', 'aspect', 'planc','profc','tpi','tpi_9','tpi_27','curvature',\n",
    "          'urban_flag', 'night_flag','region','difference']\n",
    "                            \n",
    "v_x_cop30=['cloud_flag_atm','subset_te_flag', 'N', 'E', 'h_te_skew','h_te_std','terrain_slope',\n",
    "          'coreg_bias_cop30',\n",
    "          'h_te_best_fit','pair','beam','slope', 'aspect', 'planc','profc','tpi','tpi_9','tpi_27','curvature',\n",
    "          'segment_cover', 'urban_flag', 'night_flag','region','difference']\n",
    "\n",
    "v_x_fab=['cloud_flag_atm','subset_te_flag', 'N', 'E', 'h_te_skew','h_te_std','terrain_slope',\n",
    "          'coreg_bias_fab',\n",
    "          'h_te_best_fit','pair','beam','slope', 'aspect', 'planc','profc','tpi','tpi_9','tpi_27','curvature',\n",
    "          'segment_cover', 'urban_flag', 'night_flag','region','difference']\n",
    "      \n",
    "# We do only send the elevation difference less than +-10 m and snow free condition for processing.\n",
    "\n",
    "dems.sf_qc = {'dtm1':' (-10<dh_after_dtm1 < 10) & brightness_flag == 0',\n",
    "              'dtm10':'(-10<dh_after_dtm10 < 10) & brightness_flag == 0',\n",
    "              'cop30':'(-10<dh_after_cop30 < 10) & brightness_flag == 0',\n",
    "              'fab':'(-10<dh_after_fab < 10) & brightness_flag == 0'}\n",
    "\n",
    "# settubf paraneters for processing\n",
    "dems.params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'n_estimators': 350,\n",
    "                'max_depth': 10,\n",
    "                'learning_rate': 0.1, # the lower, the slower, the roboster\n",
    "                'min_child_weight': 1,\n",
    "                'subsample': 0.7,  # introduce the randomness to against overfitting\n",
    "                'colsample_bytree': 1,\n",
    "                'gamma': 0.1,\n",
    "              }\n",
    "\n",
    "# all the regression will be saved in object dems, reffering to function regression_sf_sc\n",
    "# and the regressor has been trained will be saved in the json file.\n",
    "for dem,vx in zip(['dtm1','dtm10','cop30','fab'],[v_x_dtm1,v_x_dtm10,v_x_cop30,v_x_fab]): #\n",
    "      dems.regression_sf_sc(dem=dem, \n",
    "                        v_x=vx,\n",
    "                        # use regressor_name to save the regressor, use regressor to load the regressor\n",
    "                        regressor_name=f'{dem}_abserror_250_10_qc_geosegment_feb.json',\n",
    "                        weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Optional Evaluation of Bias correction\n",
    "\n",
    "We have a tiny dataset from field trip for evaluation (only 65 points of snow depth), which is not included in the Zenodo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xsnow.goregression import evaluate_bias_correction\n",
    "\n",
    "# (1) Print the quantile of the snow depth at 0.1, 0.5, 0.995. \n",
    "# To check the negative value (which is not true for 'snow depth').\n",
    "print(dems.sc['sd_correct_dtm1'].quantile(q=[0.1,0.5,0.995]))\n",
    "print(dems.sc['snowdepth_dtm1'].quantile(q=[0.1,0.5,0.995]))\n",
    "\n",
    "# (2) a tiny ALS ground truth dataset\n",
    "p, df_sub_ = dems.validation_lidar(dem='dtm1',corrected_sd='sd_correct_dtm1',dst_res=(10,10)) # Use ground truth in resolution of 10m \n",
    "evaluate_bias_correction(dems.sc,df_sub_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xsnow.goplot\n",
    "from xsnow.goplot import plot_all_hist_sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (3) Plot the histogram for the snow free condition, at national wide.\n",
    "plot_all_hist_sf(dems.sf,std=None,perc_t=None,window=(-10,10))\n",
    "plt.savefig('dtm1_cop30_hist_snow_free_.jpg',dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analysis on the bias correction by SHAP\n",
    "\n",
    "Figures in supplementary materials are generated by the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Downwscaling Model training\n",
    "\n",
    "We want to know the snow depth distribution in the area of interest. But we do only have snow depth retrieved from the step 2 (stored in object 'dems.sc')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The area of interest / time of interest\n",
    "df_nve_08 = pd.read_csv('df_nve_08.csv')\n",
    "df_nve_08['month'] = 4\n",
    "\n",
    "bottom = df_nve_08.N.min()\n",
    "top = df_nve_08.N.max()\n",
    "left = df_nve_08.E.min()\n",
    "right = df_nve_08.E.max()\n",
    "\n",
    "# The following qc is for filtering training dataset.\n",
    "dems.sc_qc = {'dtm1':f'({bottom-100000} < N < {top+100000}) & ({left-50000} < E < {right+50000})& sd_correct_dtm1 >=0 & n_te_photons > 30 & h_te_uncertainty < 25 & {dems.era} < 12 & abs(df_dtm1_era5) < 12',\n",
    "              'dtm10':f'({bottom-100000} < N < {top+100000}) & ({left-50000} < E < {right+50000})& sd_correct_dtm10 >=0 & n_te_photons > 30 & h_te_uncertainty < 25 & {dems.era} < 12 & abs(df_dtm10_era5) < 12',\n",
    "              'cop30':f'({bottom-100000} < N < {top+100000}) & ({left-50000} < E < {right+50000})& sd_correct_cop30 >=0 & n_te_photons > 30 & h_te_uncertainty < 25 & {dems.era} < 12 & abs(df_cop30_era5) < 12',\n",
    "              'fab':f'({bottom-100000} < N < {top+100000}) & ({left-50000} < E < {right+50000})& sd_correct_fab >=0 & n_te_photons > 30 & h_te_uncertainty < 25 & {dems.era} < 12 & abs(df_fab_era5) < 12'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combination of sc and sf. We combine it because we want the model are traning for all season dataset, not just for specific month. \n",
    "# Note that the sc and sf should have the same columns. So the following code is to make sure that the columns are the same.\n",
    "\n",
    "# Iterate through each DEM\n",
    "for dem_name in ['dtm1', 'dtm10', 'cop30', 'fab']:\n",
    "    # Calculate the difference between the corrected snow depth and ERA5\n",
    "    dems.sc[f'df_{dem_name}_era'] = dems.sc[f'sd_correct_{dem_name}'] - dems.sf[dems.era]\n",
    "\n",
    "# combine with sf\n",
    "# append new columns\n",
    "dems.sf['snowdepth_cop30'] = dems.sf['snowdepth_fab'] = dems.sf['snowdepth_dtm1'] = dems.sf['snowdepth_dtm10'] = 0\n",
    "dems.sc[['dh_after_dtm1','dh_after_dtm10','dh_after_fab','dh_after_cop30']] = dems.sc[['snowdepth_dtm1','snowdepth_dtm10','snowdepth_fab','snowdepth_cop30']]\n",
    "\n",
    "# when df = sd_correct - sd_era. If you wanna correct sd_era, here shoule be the following\n",
    "dems.sf['df_dtm1_era5'] = dems.sf['df_dtm10_era5'] = dems.sf['df_fab_era5'] = dems.sf['df_cop30_era5'] = 0 - dems.sf[dems.era]\n",
    "dems.sf['sd_correct_dtm1'] = dems.sf['sd_correct_dtm10'] = dems.sf['sd_correct_cop30'] = dems.sf['sd_correct_fab'] = 0\n",
    "dems.sf['subset_te_flag'] = 5\n",
    "dems.sf['month'] = pd.DatetimeIndex(dems.sf['date_']).month\n",
    "dems.sc['month'] = pd.DatetimeIndex(dems.sc['date_']).month\n",
    "\n",
    "# Find shared columns\n",
    "shared_columns = dems.sc.columns.intersection(dems.sf.columns)\n",
    "# Combine and keep shared columns\n",
    "combine_sc = pd.concat([dems.sc[shared_columns], dems.sf[shared_columns]], axis=0, ignore_index=True)\n",
    "dems.sc = combine_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the downscaling regression.\n",
    "dems.params_2 = {\n",
    "                'objective': 'reg:absoluteerror',\n",
    "                'max_depth': 10,\n",
    "                'n_estimators':350,\n",
    "                'learning_rate': 0.1,\n",
    "                'min_child_weight': 1,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 1,\n",
    "                'gamma': 0.1,\n",
    "                'verbose_eval':True,\n",
    "            }\n",
    "# Features for the downscaling regression.\n",
    "v_x=['E','N','h_te_best_fit', 'slope', 'aspect', 'planc','profc',  # 'sde_se'\n",
    "     'tpi','tpi_9','tpi_27','curvature','sde_era','wf_positive','wf_negative','month']\n",
    "\n",
    "\n",
    "for dem in ['dtm1','dtm10','cop30','fab']:\n",
    "     dems.regression_sc_sd_predict(dem=dem,\n",
    "                                   raw_pts=df_nve_08,\n",
    "                                   v_x=v_x,\n",
    "                                   regressor_name=f'sd_{dem}_abserror_250_10_qc_nve_geosegment_abs_feb.json',\n",
    "                                   regression='OTHERS',\n",
    "                                   weight=None);\n",
    "\n",
    "# Untill now, we finised the traning of the model. We can use it for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 optional evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which snow depth from ERA has been donwscaled\n",
    "print(dems.era)\n",
    "\n",
    "# For 7127 points, we know the ground truth of snow depth from NVE's ALS. So that we can check the performance of the model.\n",
    "from xsnow.goplot import plot_scater_nve\n",
    "v_dtm1 =plot_scater_nve(df_nve_08.query('sd_nve_10 > 0 & sd_predict_dtm1 > -1'), 'sd_nve_10', 'sd_predict_dtm1', y_offset=1,title='10 m NVE')\n",
    "v_dtm1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, we can check the performance of the model by using the histogram.\n",
    "# Where we can see the distribution of the snow depth from NVE and the downscaling model.\n",
    "# the downscaling model has much less extreame value\n",
    "\n",
    "from xsnow.goregression import evaluate_downscaling\n",
    "from xsnow.goplot import final_histogram\n",
    "final_histogram(df_nve_08.query('sd_nve_10>0 & profc > -20')['sd_nve_10'],df_nve_08.query('sd_nve_10>0 & profc > -20')['sd_predict_dtm1']-1,dH_ref=df_nve_08.query('sd_nve_20>0')['sd_nve_20'],range=(0,8),window=(0,8),legend=['NVE 10','downscaling'],title='snow depth');\n",
    "evaluate_downscaling(df_nve_08.query('sd_nve_10>0 & profc > -20'),cols=['sd_predict_dtm1'],offset=1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xdem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
